{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vWXalXMQbwK",
        "outputId": "9184eb42-dcdf-49a3-d70d-1e790c63c810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-21 02:16:52--  https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.3/llamafile-0.9.3\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/689773665/505fecd2-528e-43f8-b311-1361aa8916be?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-21T02%3A51%3A16Z&rscd=attachment%3B+filename%3Dllamafile-0.9.3&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-21T01%3A50%3A57Z&ske=2025-08-21T02%3A51%3A16Z&sks=b&skv=2018-11-09&sig=UE0X%2Bfr71so7dENFdwOb%2FwL4uQlKSiQrwXOdyBfAYMs%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NTc0MjkxMiwibmJmIjoxNzU1NzQyNjEyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.s5M28Y2fOngZuEWeGpF9AWOHAmjeOVwRnKcZQeI3VLI&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.9.3&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-21 02:16:52--  https://release-assets.githubusercontent.com/github-production-release-asset/689773665/505fecd2-528e-43f8-b311-1361aa8916be?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-21T02%3A51%3A16Z&rscd=attachment%3B+filename%3Dllamafile-0.9.3&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-21T01%3A50%3A57Z&ske=2025-08-21T02%3A51%3A16Z&sks=b&skv=2018-11-09&sig=UE0X%2Bfr71so7dENFdwOb%2FwL4uQlKSiQrwXOdyBfAYMs%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NTc0MjkxMiwibmJmIjoxNzU1NzQyNjEyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.s5M28Y2fOngZuEWeGpF9AWOHAmjeOVwRnKcZQeI3VLI&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.9.3&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 307741491 (293M) [application/octet-stream]\n",
            "Saving to: ‘llamafile’\n",
            "\n",
            "llamafile           100%[===================>] 293.48M   144MB/s    in 2.0s    \n",
            "\n",
            "2025-08-21 02:16:54 (144 MB/s) - ‘llamafile’ saved [307741491/307741491]\n",
            "\n",
            "--2025-08-21 02:16:54--  https://sourceforge.net/projects/llamafile.mirror/files/0.9.3/zipalign-0.9.3/download\n",
            "Resolving sourceforge.net (sourceforge.net)... 104.18.13.149, 104.18.12.149, 2606:4700::6812:d95, ...\n",
            "Connecting to sourceforge.net (sourceforge.net)|104.18.13.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://downloads.sourceforge.net/project/llamafile.mirror/0.9.3/zipalign-0.9.3?ts=gAAAAABopoGWJBURa8m7gbs3cf5Zi9Bsw2hcBxmN7JjmBPI_gdeZZZjoKICzKC7hg6MZbpgTmZcQWq9S1jXPImWNKPA-_7PNxA%3D%3D&use_mirror=master&r= [following]\n",
            "--2025-08-21 02:16:54--  https://downloads.sourceforge.net/project/llamafile.mirror/0.9.3/zipalign-0.9.3?ts=gAAAAABopoGWJBURa8m7gbs3cf5Zi9Bsw2hcBxmN7JjmBPI_gdeZZZjoKICzKC7hg6MZbpgTmZcQWq9S1jXPImWNKPA-_7PNxA%3D%3D&use_mirror=master&r=\n",
            "Resolving downloads.sourceforge.net (downloads.sourceforge.net)... 104.18.13.149, 104.18.12.149, 2606:4700::6812:c95, ...\n",
            "Connecting to downloads.sourceforge.net (downloads.sourceforge.net)|104.18.13.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://master.dl.sourceforge.net/project/llamafile.mirror/0.9.3/zipalign-0.9.3?viasf=1 [following]\n",
            "--2025-08-21 02:16:55--  https://master.dl.sourceforge.net/project/llamafile.mirror/0.9.3/zipalign-0.9.3?viasf=1\n",
            "Resolving master.dl.sourceforge.net (master.dl.sourceforge.net)... 216.105.38.12\n",
            "Connecting to master.dl.sourceforge.net (master.dl.sourceforge.net)|216.105.38.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 694446 (678K) [application/octet-stream]\n",
            "Saving to: ‘zipalign’\n",
            "\n",
            "zipalign            100%[===================>] 678.17K  2.56MB/s    in 0.3s    \n",
            "\n",
            "2025-08-21 02:16:59 (2.56 MB/s) - ‘zipalign’ saved [694446/694446]\n",
            "\n",
            "--2025-08-21 02:16:59--  https://huggingface.co/unsloth/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-UD-IQ3_XXS.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.170.185.35, 3.170.185.33, 3.170.185.14, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.170.185.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/680f8fa2384885553e2167cf/a1b9abf7f91a6df9e944ea5499b800026f16ea0e917e289740f785c54675222d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250821%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250821T021659Z&X-Amz-Expires=3600&X-Amz-Signature=c134ad03369f4afc805ae5382b55d9367047bce97d0a3f4772ed063762db8f9e&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Qwen3-8B-UD-IQ3_XXS.gguf%3B+filename%3D%22Qwen3-8B-UD-IQ3_XXS.gguf%22%3B&x-id=GetObject&Expires=1755746219&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTc0NjIxOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODBmOGZhMjM4NDg4NTU1M2UyMTY3Y2YvYTFiOWFiZjdmOTFhNmRmOWU5NDRlYTU0OTliODAwMDI2ZjE2ZWEwZTkxN2UyODk3NDBmNzg1YzU0Njc1MjIyZCoifV19&Signature=WKlu5jQu3Hm6RYTtXWll9NxoKi3DgjEKuHQirybaWbMoHtDH8471yAeaZBQ6EXwOgQpO4sYeld5vMaJpPZzzMl9q8DC%7Epl2ZhD536osLfWJLSPv-eWp3rxkxuMT-CcjtycH%7EGCU0fpLXuDEL-oQ%7EasqDQXLVVrihuIKuOiiI9YMYRX3AshwyeSONRPjWfVjiwO20T74zgSLEisZJ-OFi61rjFJr0LmTObjpnEro9C%7EMVp6JRpVv0xT77QPDvePUqWnVAE0u0%7EP7kwR1uTcedcoZ5YuPPkUIIGaxTMHbuWsCUBY%7EsGwDvlCJOHVDCjp2T2leZSScuMMLexjIQJd6ovg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-21 02:16:59--  https://cas-bridge.xethub.hf.co/xet-bridge-us/680f8fa2384885553e2167cf/a1b9abf7f91a6df9e944ea5499b800026f16ea0e917e289740f785c54675222d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250821%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250821T021659Z&X-Amz-Expires=3600&X-Amz-Signature=c134ad03369f4afc805ae5382b55d9367047bce97d0a3f4772ed063762db8f9e&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Qwen3-8B-UD-IQ3_XXS.gguf%3B+filename%3D%22Qwen3-8B-UD-IQ3_XXS.gguf%22%3B&x-id=GetObject&Expires=1755746219&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTc0NjIxOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODBmOGZhMjM4NDg4NTU1M2UyMTY3Y2YvYTFiOWFiZjdmOTFhNmRmOWU5NDRlYTU0OTliODAwMDI2ZjE2ZWEwZTkxN2UyODk3NDBmNzg1YzU0Njc1MjIyZCoifV19&Signature=WKlu5jQu3Hm6RYTtXWll9NxoKi3DgjEKuHQirybaWbMoHtDH8471yAeaZBQ6EXwOgQpO4sYeld5vMaJpPZzzMl9q8DC%7Epl2ZhD536osLfWJLSPv-eWp3rxkxuMT-CcjtycH%7EGCU0fpLXuDEL-oQ%7EasqDQXLVVrihuIKuOiiI9YMYRX3AshwyeSONRPjWfVjiwO20T74zgSLEisZJ-OFi61rjFJr0LmTObjpnEro9C%7EMVp6JRpVv0xT77QPDvePUqWnVAE0u0%7EP7kwR1uTcedcoZ5YuPPkUIIGaxTMHbuWsCUBY%7EsGwDvlCJOHVDCjp2T2leZSScuMMLexjIQJd6ovg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.169.149.55, 3.169.149.57, 3.169.149.100, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.169.149.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3410265920 (3.2G)\n",
            "Saving to: ‘qwen3-8b.gguf’\n",
            "\n",
            "qwen3-8b.gguf       100%[===================>]   3.18G   302MB/s    in 11s     \n",
            "\n",
            "2025-08-21 02:17:10 (289 MB/s) - ‘qwen3-8b.gguf’ saved [3410265920/3410265920]\n",
            "\n",
            "used = 124'216\n",
            "note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading\n",
            "Log start\n",
            "main: build = 1500 (a30b324)\n",
            "main: built with cosmocc (GCC) 11.2.0 for x86_64-linux-cosmo\n",
            "main: seed  = 1755742638\n",
            "llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from qwen3-8b.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen3-8B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = Qwen3-8B\n",
            "llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\n",
            "llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960\n",
            "llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 12288\n",
            "llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  27:                          general.file_type u32              = 23\n",
            "llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-8B-GGUF/imatrix_unsloth.dat\n",
            "llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-8B.txt\n",
            "llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 252\n",
            "llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685\n",
            "llama_model_loader: - type  f32:  145 tensors\n",
            "llama_model_loader: - type q3_K:    1 tensors\n",
            "llama_model_loader: - type q4_K:   36 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq2_xs:   10 tensors\n",
            "llama_model_loader: - type iq3_xxs:  113 tensors\n",
            "llama_model_loader: - type iq3_s:   36 tensors\n",
            "llama_model_loader: - type iq2_s:   52 tensors\n",
            "llama_model_loader: - type iq4_xs:    5 tensors\n",
            "llm_load_vocab: special tokens cache size = 26\n",
            "llm_load_vocab: token to piece cache size = 0.9311 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen3\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 151936\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 40960\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 36\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 12288\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 40960\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = IQ3_XXS - 3.0625 bpw\n",
            "llm_load_print_meta: model params     = 8.19 B\n",
            "llm_load_print_meta: model size       = 3.17 GiB (3.33 BPW) \n",
            "llm_load_print_meta: general.name     = Qwen3-8B\n",
            "llm_load_print_meta: BOS token        = 11 ','\n",
            "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 151654 '<|vision_pad|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.21 MiB\n",
            "\u001b[Kllm_load_tensors:        CPU buffer size =  3246.60 MiB\n",
            "..................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1152.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1302\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 6 (n_threads_batch = 6) / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France?\n",
            "\n",
            "Okay, I need to figure out the answer to the question \"What is the capital of France?\" But the user has repeated the question multiple times. Let me think. Well, the capital of France is Paris. But why is the user asking it so many times? Maybe they are testing me or trying to see if I can answer repeatedly. But I should answer once and then explain the repetition. Wait, the user might be confused or have a typo. Let me confirm. France's capital is indeed Paris. The repetition could be due to a mistake or\n",
            "\n",
            "llama_print_timings:        load time =    2688.27 ms\n",
            "llama_print_timings:      sample time =      21.15 ms /   175 runs   (    0.12 ms per token,  8273.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3028.58 ms /     7 tokens (  432.65 ms per token,     2.31 tokens per second)\n",
            "llama_print_timings:        eval time =   82339.34 ms /   174 runs   (  473.21 ms per token,     2.11 tokens per second)\n",
            "llama_print_timings:       total time =   85586.70 ms /   181 tokens\n"
          ]
        }
      ],
      "source": [
        "# get the runner + zipalign\n",
        "!wget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.3/llamafile-0.9.3 -O llamafile\n",
        "!chmod +x llamafile\n",
        "!wget https://sourceforge.net/projects/llamafile.mirror/files/0.9.3/zipalign-0.9.3/download -O zipalign\n",
        "!chmod +x zipalign\n",
        "\n",
        "# download your GGUF\n",
        "!wget \"https://huggingface.co/unsloth/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-UD-IQ3_XXS.gguf?download=true\" -O qwen3-8b.gguf\n",
        "\n",
        "# write default args file (so llamafile knows what to load by default)\n",
        "!printf \"%s\\n%s\\n\" \"-m\" \"qwen3-8b.gguf\" > .args\n",
        "\n",
        "# make a copy of the runner and append model + args\n",
        "!cp llamafile qwen3-8b.llamafile\n",
        "!./zipalign -j0 qwen3-8b.llamafile qwen3-8b.gguf .args\n",
        "\n",
        "# now you have a portable binary\n",
        "!./qwen3-8b.llamafile --cli -p \"What is the capital of France?\"\n"
      ]
    }
  ]
}